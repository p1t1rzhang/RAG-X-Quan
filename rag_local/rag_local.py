# rag_local.py
from pathlib import Path
from typing import List, Tuple
import os
import sys

from langchain_ollama import ChatOllama, OllamaEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS

# å¯èƒ½ç”¨åˆ°çš„ PDF loaderï¼ˆæœ‰å°±ç”¨ã€æ²’æœ‰å°±è·³éï¼‰
from langchain_community.document_loaders import TextLoader
from langchain_community.document_loaders import PyPDFLoader  # åŸºæœ¬æ¬¾
try:
    from langchain_community.document_loaders import PyMuPDFLoader  # å‚™æ´ï¼ˆfitzï¼‰
    HAS_PYMUPDF = True
except Exception:
    HAS_PYMUPDF = False

INDEX_DIR = Path("index")
DATA_DIR = Path(os.getenv("RAG_DATA_DIR", "/Users/p1t1rzhang/Desktop/rag_local/RAG_DATA_DIR"))

# é è¨­ embedding/èŠå¤©æ¨¡å‹ï¼›å¯ç”¨ç’°å¢ƒè®Šæ•¸è¦†è“‹
EMBED_MODEL = os.getenv("EMBED_MODEL", "bge-m3:latest")  # è‹¥æ²’æ‹‰éï¼Œå¯æ”¹æˆ "nomic-embed-text"
CHAT_MODEL = os.getenv("CHAT_MODEL", "gpt-oss:20b")  # å…ˆç”¨ä½ æœ¬æ©Ÿç¾æˆçš„ï¼›ä¹‹å¾Œå¯åˆ‡ gpt-oss:20b


def is_pdf(path: Path) -> bool:
    return path.suffix.lower() == ".pdf"


def is_txt(path: Path) -> bool:
    return path.suffix.lower() in {".txt", ".md", ".markdown"}


def looks_like_pdf_binary(path: Path) -> bool:
    # å¿«é€Ÿæª¢æŸ¥æª”é ­æ˜¯å¦ç‚º %PDF-
    try:
        with open(path, "rb") as f:
            head = f.read(5)
        return head.startswith(b"%PDF-")
    except Exception:
        return False


def load_one_file(path: Path) -> List:
    """é€æª”è¼‰å…¥ï¼›PDF å…ˆç”¨ PyPDFLoaderï¼Œå¤±æ•—å†ç”¨ PyMuPDFLoaderï¼ˆè‹¥å¯ç”¨ï¼‰ã€‚"""
    docs = []
    try:
        if is_txt(path):
            docs.extend(TextLoader(str(path), encoding="utf-8").load())
        elif is_pdf(path):
            if not looks_like_pdf_binary(path):
                print(f"âš ï¸  è·³éï¼ˆä¸æ˜¯åˆæ³• PDF é ­ï¼‰ï¼š{path.name}")
                return docs
            try:
                docs.extend(PyPDFLoader(str(path)).load())
            except Exception as e1:
                if HAS_PYMUPDF:
                    try:
                        docs.extend(PyMuPDFLoader(str(path)).load())
                    except Exception as e2:
                        print(f"âš ï¸  PDF è§£æå¤±æ•—ï¼ˆpypdf & pymupdfï¼‰ï¼š{path.name} | {e2}")
                else:
                    print(f"âš ï¸  PDF è§£æå¤±æ•—ï¼ˆpypdfï¼‰ï¼Œä¸”æœªå®‰è£ pymupdfï¼š{path.name} | {e1}")
        # å…¶ä»–å‰¯æª”åå¿½ç•¥
    except Exception as e:
        print(f"âš ï¸  è¼‰å…¥å¤±æ•—ï¼š{path.name} | {e}")
    return docs


def load_and_split(data_dir: Path):
    if not data_dir.exists():
        raise FileNotFoundError(f"è³‡æ–™å¤¾ä¸å­˜åœ¨ï¼š{data_dir}")

    files = [p for p in data_dir.rglob("*") if p.is_file() and (is_pdf(p) or is_txt(p))]
    if not files:
        raise ValueError(f"åœ¨ {data_dir} æ²’æ‰¾åˆ° PDF/TXT æ–‡ä»¶")

    loaded_docs = []
    skipped = 0
    print(f"ğŸ“¦ æº–å‚™è¼‰å…¥ {len(files)} å€‹æª”æ¡ˆâ€¦")
    for p in sorted(files):
        docs = load_one_file(p)
        if docs:
            loaded_docs.extend(docs)
            print(f"  âœ… {p.name} -> {len(docs)} docs")
        else:
            skipped += 1
            print(f"  â­ï¸  è·³éï¼š{p.name}")

    print(f"ğŸ“Š è¼‰å…¥å®Œæˆï¼šæˆåŠŸ {len(loaded_docs)}ï¼Œè·³é {skipped}")

    splitter = RecursiveCharacterTextSplitter(chunk_size=1200, chunk_overlap=150, add_start_index=True)
    chunks = splitter.split_documents(loaded_docs)
    print(f"âœ‚ï¸  åˆ‡å¡Šå®Œæˆï¼š{len(chunks)} chunks")
    return chunks


def build_or_load_index(chunks):
    embed = OllamaEmbeddings(model=EMBED_MODEL)
    idx_path = INDEX_DIR / "faiss_index"

    faiss_bin = idx_path.with_suffix(".faiss")
    faiss_meta = idx_path.with_suffix(".pkl")
    if faiss_bin.exists() and faiss_meta.exists():
        vs = FAISS.load_local(folder_path=str(idx_path), embeddings=embed, allow_dangerous_deserialization=True)
    else:
        os.makedirs(INDEX_DIR, exist_ok=True)
        vs = FAISS.from_documents(chunks, embed)
        vs.save_local(str(idx_path))
    return vs


def answer_question(vs, question: str, k: int = 5) -> Tuple[str, List[Tuple[str, str]]]:
    # ä½¿ç”¨ MMR æª¢ç´¢é™ä½é‡è¤‡
    retriever = vs.as_retriever(search_type="mmr", search_kwargs={"k": k, "lambda_mult": 0.3})
    # é—œéµä¿®æ­£ï¼šå‚³å…¥ç´”å­—ä¸²ï¼Œä¸è¦ dict
    try:
        hits = retriever.invoke(question)
    except TypeError:
        # ç›¸å®¹èˆŠæ³•
        hits = retriever.get_relevant_documents(question)

    if not hits:
        msg = ("ã€æ²’æœ‰æª¢ç´¢åˆ°å…§å®¹ã€‘\n"
               "è«‹ç¢ºèªè³‡æ–™å¤¾å…§æœ‰å¯è®€çš„ TXT/ç„¡å¯†ç¢¼ PDFï¼›æˆ–æ›å€‹é—œéµå­—å†å•ä¸€æ¬¡ã€‚")
        return msg, []

    # æ”¶é›†ä¾†æºèˆ‡ context
    sources: List[Tuple[str, str]] = []
    context_blocks: List[str] = []
    for d in hits:
        meta = d.metadata or {}
        fname = Path(meta.get("source", "unknown")).name
        page = f"p.{meta.get('page', '')}" if meta.get("page") is not None else ""
        sources.append((fname, page))
        context_blocks.append(d.page_content)

    context_text = "\n\n---\n\n".join(context_blocks)

    llm = ChatOllama(model=CHAT_MODEL, temperature=0.2)
    system_msg = (
        "ä½ æ˜¯é‡åŒ–ç ”ç©¶åŠ©ç†ã€‚è«‹åš´æ ¼æ ¹æ“šæä¾›çš„ã€æª¢ç´¢å…§å®¹ã€‘å›ç­”ï¼›"
        "è‹¥æ–‡ä»¶æœªæåŠï¼Œå°±èªªä¸çŸ¥é“ï¼Œé¿å…å¹»è¦ºã€‚å›è¦†æœ«å°¾ä¸éœ€è¦å†æ¬¡åˆ—å‡ºä¾†æºã€‚"
    )
    user_msg = f"""æˆ‘çš„å•é¡Œï¼š{question}

ã€æª¢ç´¢å…§å®¹ã€‘
{context_text}
"""

    try:
        resp = llm.invoke([("system", system_msg), ("user", user_msg)])
        answer = resp.content if hasattr(resp, "content") else str(resp)
        return answer.strip(), sources
    except Exception as e:
        err = str(e)
        tips = (f"\n\nâŒ å‘¼å«æ¨¡å‹å¤±æ•—ï¼š{err}\n"
                f"ç›®å‰ CHAT_MODEL è¨­å®šç‚ºï¼š{CHAT_MODEL}\n"
                "è«‹å…ˆä»¥ `ollama list` ç¢ºèªæ˜¯å¦å·²ä¸‹è¼‰ï¼›\n"
                "è‹¥æ²’æœ‰è©²æ¨¡å‹ï¼š`ollama pull llama3.2:latest` æˆ–æ”¹ç”¨ `export CHAT_MODEL=\"gpt-oss:20b\"`ï¼ˆæ‹‰å¥½å¾Œå†ç”¨ï¼‰ã€‚")
        return tips, []


if __name__ == "__main__":
    print(f"ğŸ”§ CHAT_MODEL = {CHAT_MODEL} | EMBED_MODEL = {EMBED_MODEL}")
    print("ğŸ” æº–å‚™å»ºç«‹/è¼‰å…¥ç´¢å¼• ...")
    chunks = load_and_split(DATA_DIR)
    vs = build_or_load_index(chunks)

    print("âœ… å°±ç·’ï¼Œé–‹å§‹æå•ï¼ˆè¼¸å…¥ç©ºç™½è¡Œé›¢é–‹ï¼‰")
    while True:
        q = input("\nå•é¡Œ> ").strip()
        if not q:
            break
        ans, srcs = answer_question(vs, q, k=5)
        print("\n--- å›ç­” ---")
        print(ans)
        if srcs:
            print("\n(åƒè€ƒä¾†æº)", "ã€".join([f"{s[0]}{(' '+s[1]) if s[1] else ''}" for s in srcs]))